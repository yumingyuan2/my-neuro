# main_chat.py - äº‹ä»¶é©±åŠ¨é‡æ„ç‰ˆæœ¬
from ai_singing_feature import SingingSystem
from openai import OpenAI
from PIL import ImageGrab
import io
import base64
from audio_mod.audio_proucess import AudioPlayer
from audio_mod.asr_module import AudioSystem
import keyboard
import inspect
from datetime import datetime

from PyQt5.QtWidgets import QApplication
from UI.live2d_model import Live2DModel, init_live2d, dispose_live2d
import sys
import json

from config_mod.load_config import load_config
import threading
import time

import pyperclip
import pyautogui

from stream_mod.bilibili_stream import BilibiliDanmuListener

# å¯¼å…¥æƒ…ç»ªå¤„ç†å™¨
from emotion_mod.emotion_handler import EmotionHandler
from agent_mod.fc_tools import MyNuroTools

from UI.typing_box import start_gui_with_ai

from bert_mod import Bert_panduan

# å¯¼å…¥äº‹ä»¶æ€»çº¿
try:
    from UI.simple_event_bus import event_bus, Events

    HAS_EVENT_BUS = True
except ImportError:
    HAS_EVENT_BUS = False
    print("æœªæ‰¾åˆ°äº‹ä»¶æ€»çº¿ï¼Œä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼")


class MyNeuro:

    def __init__(self):
        # åˆå§‹åŒ–é…ç½®
        self.config = load_config()
        self.bert = Bert_panduan()

        # APIé…ç½®
        API_KEY = self.config['api']['api_key']
        API_URL = self.config['api']['api_url']
        self.model = self.config['api']['model']
        self.client = OpenAI(api_key=API_KEY, base_url=API_URL)

        self.messages = [{
            'role': 'system', 'content': self.config['api']['system_prompt']
        }]

        # å„ç§é…ç½®
        self.cut_text_tts = self.config['features']['cut_text_tts']
        self.interval = self.config['inputs']['auto_chat']['interval']
        self.audo_chat = self.config['inputs']['auto_chat']['enabled']
        self.asr_real_time = self.config['inputs']['asr'].get('real_time', True)

        # çŠ¶æ€æ§åˆ¶
        self.mic_enabled = True
        self.ai_is_responding = False
        self.stop_flag = False

        # åˆå§‹åŒ–Live2D
        init_live2d()
        self.app = QApplication(sys.argv)
        live_model = Live2DModel()
        live_model.show()

        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.vad_input = AudioSystem(parent_neuro=self)
        self.asr_vad = self.config['inputs']['asr']['enabled']

        # æƒ…ç»ªå¤„ç†å™¨
        self.emotion_handler = EmotionHandler(config_path="emotion_mod/emotion_actions.json", live_model=live_model)

        # éŸ³é¢‘æ’­æ”¾å™¨
        live_2d = self.config['features']['live2d']
        self.audio_player = AudioPlayer(live_model=live_model if live_2d else None,
                                        emotion_handler=self.emotion_handler)

        # å”±æ­Œç³»ç»Ÿ
        self.singing_system = SingingSystem(
            live_model=live_model if live_2d else None,
            audio_dir="KTV/output"
        )

        # Function calling
        self.function_calling_enabled = self.config['features']['function_calling']
        if self.function_calling_enabled:
            self.fc_tool = MyNuroTools(self)
        else:
            self.fc_tool = None

        # å“”å“©å“”å“©ç›´æ’­
        self.listener = BilibiliDanmuListener()

        # å¿«æ·é”®
        keyboard.add_hotkey('ctrl+i', self.stop_key)

        # ğŸ”¥ æ–°å¢ï¼šäº‹ä»¶é©±åŠ¨é›†æˆ
        if HAS_EVENT_BUS:
            self._setup_event_handlers()

    def _setup_event_handlers(self):
        """è®¾ç½®äº‹ä»¶å¤„ç†å™¨ - æ–°å¢"""
        # è®¢é˜…ç”¨æˆ·è¾“å…¥äº‹ä»¶
        event_bus.subscribe(Events.USER_INPUT, self._handle_user_input_event)

        # è®¢é˜…éŸ³é¢‘æ§åˆ¶äº‹ä»¶
        event_bus.subscribe(Events.AUDIO_INTERRUPT, self._handle_audio_interrupt_event)
        event_bus.subscribe(Events.MIC_TOGGLE, self._handle_mic_toggle_event)

        # è®¢é˜…AIå“åº”äº‹ä»¶
        event_bus.subscribe("ai_response_start", self._handle_ai_response_start)
        event_bus.subscribe("ai_response_end", self._handle_ai_response_end)

    def _handle_user_input_event(self, data):
        """å¤„ç†ç”¨æˆ·è¾“å…¥äº‹ä»¶ - æ–°å¢"""
        user_text = data.get('text', '')
        source = data.get('source', 'unknown')

        print(f"[äº‹ä»¶] æ”¶åˆ°ç”¨æˆ·è¾“å…¥: {user_text} (æ¥æº: {source})")
        self.start_chat(user_text)

    def _handle_audio_interrupt_event(self, data=None):
        """å¤„ç†éŸ³é¢‘æ‰“æ–­äº‹ä»¶ - æ–°å¢"""
        print("[äº‹ä»¶] æ”¶åˆ°éŸ³é¢‘æ‰“æ–­ä¿¡å·")
        self.stop_key()

    def _handle_mic_toggle_event(self, data):
        """å¤„ç†éº¦å…‹é£å¼€å…³äº‹ä»¶ - æ–°å¢"""
        enabled = data.get('enabled', True)
        self.set_mic_enabled(enabled)

    def _handle_ai_response_start(self, data=None):
        """å¤„ç†AIå¼€å§‹å“åº”äº‹ä»¶ - æ–°å¢"""
        if not self.asr_real_time:
            self.set_mic_enabled(False)
            print("ğŸ”‡ [äº‹ä»¶] éº¦å…‹é£å·²å…³é—­ï¼ŒAIå›å¤ä¸­...")

    def _handle_ai_response_end(self, data=None):
        """å¤„ç†AIå“åº”ç»“æŸäº‹ä»¶ - æ–°å¢"""
        if not self.asr_real_time:
            self.wait_for_audio_finish()
            self.set_mic_enabled(True)
            print("ğŸ¤ [äº‹ä»¶] éº¦å…‹é£å·²å¼€å¯ï¼Œå¯ä»¥è¯´è¯äº†")

    # æ–°å¢ï¼šäº‹ä»¶å‘å¸ƒæ–¹æ³•
    def publish_event(self, event_name, data=None):
        """å‘å¸ƒäº‹ä»¶"""
        if HAS_EVENT_BUS:
            event_bus.publish(event_name, data)

    # ğŸ”¥ ä¿®æ”¹åŸæœ‰æ–¹æ³•ï¼Œé›†æˆäº‹ä»¶å‘å¸ƒ
    def start_chat(self, user):
        """å¼€å§‹èŠå¤© - ä¿®æ”¹ä¸ºæ”¯æŒäº‹ä»¶"""
        self.stop_flag = False

        # å‘å¸ƒå¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥äº‹ä»¶
        self.publish_event("user_input_processing", {"text": user})

        # åŸæœ‰é€»è¾‘
        data = self.bert.vl_bert(user)
        if data == 'æ˜¯':
            image_data = self.get_image_base64()
            self.add_vl_message(user, image_data)
        else:
            self.add_message('user', user)

        # å‘å¸ƒAIå¼€å§‹å“åº”äº‹ä»¶
        self.publish_event("ai_response_start")

        response = self.get_requests()
        ai_response = self.accept_chat(response)

        if ai_response:
            self.add_message('assistant', ai_response)

        # å‘å¸ƒAIå“åº”ç»“æŸäº‹ä»¶
        self.publish_event("ai_response_end")

    def accept_chat(self, response):
        """æ¥æ”¶èŠå¤© - ä¿®æ”¹ä¸ºæ”¯æŒäº‹ä»¶"""
        # åŸæœ‰çš„AIå›å¤é€»è¾‘...
        if self.function_calling_enabled and self.fc_tool:
            result = self.fc_tool.accept_chat(response)
            if self.cut_text_tts and not self.stop_flag:
                self.audio_player.finish_current_text()
            self.ai_is_responding = False
            print("ğŸ”¥ğŸ”¥ğŸ”¥ AIå›å¤ç»“æŸï¼ğŸ”¥ğŸ”¥ğŸ”¥")
            return result
        else:
            full_assistant = ''
            print('AI:', end='')

            for chunk in response:
                if self.stop_flag:
                    print("ğŸ”¥ æ”¶åˆ°æ‰“æ–­ä¿¡å·ï¼Œåœæ­¢AIå›å¤")
                    # å‘å¸ƒæ‰“æ–­äº‹ä»¶
                    self.publish_event(Events.AUDIO_INTERRUPT)
                    break

                if chunk.choices and chunk.choices[0].delta.content is not None:
                    ai_response = chunk.choices[0].delta.content
                    print(ai_response, end='', flush=True)

                    # å‘å¸ƒAIå“åº”å—äº‹ä»¶
                    self.publish_event(Events.AI_RESPONSE, {
                        "chunk": ai_response,
                        "full_text": full_assistant + ai_response
                    })

                    if self.cut_text_tts:
                        self.audio_player.cut_text(ai_response)

                    full_assistant += ai_response
                    time.sleep(0.05)

            if self.cut_text_tts and not self.stop_flag:
                self.audio_player.finish_current_text()

            print()
            self.ai_is_responding = False
            self.stop_flag = False
            self.emotion_handler.reset_buffer()
            print("ğŸ”¥ğŸ”¥ğŸ”¥ AIå›å¤ç»“æŸï¼ğŸ”¥ğŸ”¥ğŸ”¥")
            return full_assistant

    def stop_key(self):
        """åœæ­¢æŒ‰é”® - ä¿®æ”¹ä¸ºæ”¯æŒäº‹ä»¶"""
        self.stop_flag = True
        self.ai_is_responding = False
        print('æ‰“æ–­ï¼')

        # å‘å¸ƒç³»ç»Ÿæ‰“æ–­äº‹ä»¶
        self.publish_event(Events.AUDIO_INTERRUPT)

        # é‡ç½®æƒ…ç»ªå¤„ç†å™¨çš„ç¼“å†²åŒº
        self.emotion_handler.reset_buffer()

    def set_mic_enabled(self, enabled):
        """æ§åˆ¶éº¦å…‹é£å¼€å…³ - ä¿®æ”¹ä¸ºæ”¯æŒäº‹ä»¶"""
        self.mic_enabled = enabled
        if hasattr(self, 'vad_input'):
            self.vad_input.set_mic_enabled(enabled)

        # å‘å¸ƒéº¦å…‹é£çŠ¶æ€äº‹ä»¶
        self.publish_event(Events.MIC_TOGGLE, {"enabled": enabled})

    # ğŸ”¥ æ–°å¢ï¼šå¤–éƒ¨è¾“å…¥æ¥å£ï¼ˆæ”¯æŒäº‹ä»¶é©±åŠ¨ï¼‰
    def handle_keyboard_input(self, text):
        """å¤„ç†é”®ç›˜è¾“å…¥"""
        self.publish_event(Events.USER_INPUT, {
            "text": text,
            "source": "keyboard"
        })

    def handle_voice_input(self, text):
        """å¤„ç†è¯­éŸ³è¾“å…¥"""
        self.publish_event(Events.USER_INPUT, {
            "text": text,
            "source": "voice"
        })

    def handle_danmu_input(self, text, nickname):
        """å¤„ç†å¼¹å¹•è¾“å…¥"""
        self.publish_event(Events.USER_INPUT, {
            "text": f"å¼¹å¹•æ¶ˆæ¯ï¼š{nickname}: {text}",
            "source": "danmu",
            "nickname": nickname
        })

    # ä¿æŒåŸæœ‰æ–¹æ³•ä¸å˜
    def wait_for_audio_finish(self):
        """ç­‰å¾…æ‰€æœ‰éŸ³é¢‘æ’­æ”¾å®Œæˆ"""
        import pygame
        while pygame.mixer.get_init() and pygame.mixer.music.get_busy():
            time.sleep(0.1)
        time.sleep(0.2)

    def add_message(self, role, content):
        self.messages.append({
            'role': role,
            'content': content
        })
        if len(self.messages) > 31:
            self.messages.pop(1)

    def get_requests(self):
        if self.function_calling_enabled and self.fc_tool:
            return self.fc_tool.get_requests()
        else:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=self.messages,
                stream=True
            )
            return response

    def get_image_base64(self):
        """æˆªå›¾å¹¶æŠŠé€šè¿‡base64å°†å›¾ç‰‡è§£ææˆäºŒè¿›åˆ¶å›¾ç‰‡æ•°æ®"""
        screenshot = ImageGrab.grab()
        buffer = io.BytesIO()
        screenshot.save(buffer, format='JPEG')
        image_data = base64.b64encode(buffer.getvalue()).decode('utf-8')
        print('æˆªå›¾')
        return image_data

    def add_vl_message(self, content, image_data):
        self.messages.append({
            'role': 'user',
            'content': [
                {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{image_data}'}},
                {'type': 'text', 'text': content}
            ]
        })
        if len(self.messages) > 31:
            self.messages.pop(1)

    # ğŸ”¥ ä¿®æ”¹è¾“å…¥å¤„ç†æ–¹æ³•ï¼Œä½¿ç”¨äº‹ä»¶é©±åŠ¨
    def asr_vad_chat(self):
        """ASRè¯­éŸ³è¾“å…¥ - ä¿®æ”¹ä¸ºäº‹ä»¶é©±åŠ¨"""
        if self.asr_vad:
            while True:
                print('å¯åŠ¨ASR')
                user = self.vad_input.vad_asr()
                if user.strip():  # åªæœ‰éç©ºè¾“å…¥æ‰å¤„ç†
                    # ä½¿ç”¨äº‹ä»¶é©±åŠ¨æ–¹å¼
                    self.handle_voice_input(user)

    def main(self):
        """GUIè¾“å…¥å¤„ç† - ä¿®æ”¹ä¸ºäº‹ä»¶é©±åŠ¨"""

        def process_keyboard_input(text):
            self.handle_keyboard_input(text)

        sys.exit(start_gui_with_ai(process_keyboard_input))

    def start_main(self):
        """å¼¹å¹•ç›‘å¬ - ä¿®æ”¹ä¸ºäº‹ä»¶é©±åŠ¨"""
        print('å¼€å§‹å¯¹è¯')
        self.listener.start_listening()

        while True:
            chat = self.listener.get_chat()
            if chat:
                # ä½¿ç”¨äº‹ä»¶é©±åŠ¨æ–¹å¼
                user_message = chat['text']
                nickname = chat['nickname']
                print(f"æ”¶åˆ°å¼¹å¹•: {nickname}: {user_message}")

                self.handle_danmu_input(user_message, nickname)

            time.sleep(1)

    def auto_chat(self):
        """è‡ªåŠ¨èŠå¤© - ä¿®æ”¹ä¸ºäº‹ä»¶é©±åŠ¨"""
        if self.audo_chat:
            while True:
                jiange = self.interval
                time.sleep(jiange)

                user = self.config['api']['auto_content_chat']
                # ä½¿ç”¨äº‹ä»¶é©±åŠ¨æ–¹å¼
                self.publish_event(Events.USER_INPUT, {
                    "text": user,
                    "source": "auto_chat"
                })

    def main_chat(self):
        """ä¸»èŠå¤©å¾ªç¯ - ä¿æŒä¸å˜"""
        threading.Thread(target=self.auto_chat, daemon=True).start()
        threading.Thread(target=self.start_main, daemon=True).start()
        threading.Thread(target=self.asr_vad_chat, daemon=True).start()

        # ä¸»çº¿ç¨‹
        if self.config['inputs']['keyboard']['enabled']:
            self.main()
        else:
            while True:
                user = input('ä½ ï¼š')
                self.handle_keyboard_input(user)


if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨äº‹ä»¶é©±åŠ¨ç‰ˆæœ¬çš„MyNeuro")
    my_neuro = MyNeuro()
    my_neuro.main_chat()

from ai_singing_feature import SingingSystem
from openai import OpenAI
from PIL import ImageGrab
import io
import base64
from audio_mod.audio_proucess import AudioPlayer
from audio_mod.asr_module import AudioSystem
import keyboard
import inspect
from datetime import datetime

from PyQt5.QtWidgets import QApplication
from UI.live2d_model import Live2DModel, init_live2d, dispose_live2d
import sys
import json

from config_mod.load_config import load_config
import threading
import time

import pyperclip
import pyautogui

from stream_mod.bilibili_stream import BilibiliDanmuListener

# å¯¼å…¥æƒ…ç»ªå¤„ç†å™¨
from emotion_mod.emotion_handler import EmotionHandler
from agent_mod.fc_tools import MyNuroTools

from UI.typing_box import start_gui_with_ai

from bert_mod import Bert_panduan


class MyNeuro:

    def __init__(self):
        # åˆå§‹åŒ–
        init_live2d()
        self.app = QApplication(sys.argv)
        live_model = Live2DModel()
        live_model.show()

        self.config = load_config()
        self.bert = Bert_panduan()

        API_KEY = self.config['api']['api_key']
        API_URL = self.config['api']['api_url']
        self.model = self.config['api']['model']

        self.client = OpenAI(api_key=API_KEY, base_url=API_URL)

        self.messages = [{
            'role': 'system', 'content': self.config['api']['system_prompt']
        }]

        self.cut_text_tts = self.config['features']['cut_text_tts']

        # æ—¶é—´é™åˆ¶
        self.interval = self.config['inputs']['auto_chat']['interval']
        self.audo_chat = self.config['inputs']['auto_chat']['enabled']

        # æ–°å¢ï¼šè·å–ASRç›‘å¬æ¨¡å¼é…ç½®
        self.asr_real_time = self.config['inputs']['asr'].get('real_time', True)

        # æ–°å¢ï¼šéº¦å…‹é£çŠ¶æ€æ§åˆ¶
        self.mic_enabled = True

        # AIå›å¤çŠ¶æ€æ ‡å¿—
        self.ai_is_responding = False

        # åˆ¤æ–­ASRæ˜¯å¦
        self.vad_input = AudioSystem(parent_neuro=self)
        self.asr_vad = self.config['inputs']['asr']['enabled']

        # æ ¹æ®configé…ç½®æ–‡ä»¶å¸ƒå°”å€¼åˆ¤æ–­æ˜¯å¦å¼€å¯live2dçš„çš®å¥—æ˜¾ç¤º
        live_2d = self.config['features']['live2d']

        # ğŸ¯ å”¯ä¸€çš„ä¿®æ”¹ï¼šåˆå§‹åŒ–æƒ…ç»ªå¤„ç†å™¨
        self.emotion_handler = EmotionHandler(config_path="emotion_mod/emotion_actions.json", live_model=live_model)

        # ğŸ¯ å”¯ä¸€çš„ä¿®æ”¹ï¼šä¼ å…¥emotion_handler
        self.audio_player = AudioPlayer(live_model=live_model,
                                        emotion_handler=self.emotion_handler) if live_2d else AudioPlayer(
            emotion_handler=self.emotion_handler)


        self.stop_flag = False
        keyboard.add_hotkey('ctrl+i', self.stop_key)

        self.singing_system = SingingSystem(
            live_model=live_model if live_2d else None,
            audio_dir="KTV/output"
        )


        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯ç”¨ function calling
        self.function_calling_enabled = self.config['features']['function_calling']
        if self.function_calling_enabled:
            self.fc_tool = MyNuroTools(self)
        else:
            self.fc_tool = None

        # å“”å“©å“”å“©çš„ç›´æ’­
        self.listener = BilibiliDanmuListener()

    def set_mic_enabled(self, enabled):
        """æ§åˆ¶éº¦å…‹é£å¼€å…³"""
        self.mic_enabled = enabled
        if hasattr(self, 'vad_input'):
            self.vad_input.set_mic_enabled(enabled)

    def wait_for_audio_finish(self):
        """ç­‰å¾…æ‰€æœ‰éŸ³é¢‘æ’­æ”¾å®Œæˆ"""
        import pygame
        while pygame.mixer.get_init() and pygame.mixer.music.get_busy():
            time.sleep(0.1)
        # é¢å¤–ç­‰å¾…ä¸€ç‚¹æ—¶é—´ç¡®ä¿å®Œå…¨ç»“æŸ
        time.sleep(0.2)

    def stop_key(self):
        self.stop_flag = True
        self.ai_is_responding = False
        print('æ‰“æ–­ï¼')
        # é‡ç½®æƒ…ç»ªå¤„ç†å™¨çš„ç¼“å†²åŒº
        self.emotion_handler.reset_buffer()

    def add_message(self, role, content):
        self.messages.append({
            'role': role,
            'content': content
        })

        if len(self.messages) > 31:
            self.messages.pop(1)

    def get_requests(self):
        # å¦‚æœå¯ç”¨äº†function callingï¼Œå°±ç”¨fc_toolçš„æ–¹æ³•
        if self.function_calling_enabled and self.fc_tool:
            return self.fc_tool.get_requests()
        else:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=self.messages,
                stream=True
            )
            return response

    def get_image_base64(self):
        """
        æˆªå›¾å¹¶æŠŠé€šè¿‡base64å°†å›¾ç‰‡è§£ææˆäºŒè¿›åˆ¶å›¾ç‰‡æ•°æ®
        """
        screenshot = ImageGrab.grab()
        buffer = io.BytesIO()
        screenshot.save(buffer, format='JPEG')
        image_data = base64.b64encode(buffer.getvalue()).decode('utf-8')
        print('æˆªå›¾')
        return image_data

    def add_vl_message(self, content, image_data):
        self.messages.append({
            'role': 'user',
            'content': [
                {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{image_data}'}},
                {'type': 'text', 'text': content}
            ]
        })

        if len(self.messages) > 31:
            self.messages.pop(1)

    def accept_chat(self, response):
        # éå®æ—¶æ¨¡å¼ï¼šAIå¼€å§‹å›å¤æ—¶å…³é—­éº¦å…‹é£
        if not self.asr_real_time:
            self.set_mic_enabled(False)
            print("ğŸ”‡ éº¦å…‹é£å·²å…³é—­ï¼ŒAIå›å¤ä¸­...")

        # è®¾ç½®AIæ­£åœ¨å›å¤çŠ¶æ€
        self.ai_is_responding = True

        # å¦‚æœå¯ç”¨äº†function callingï¼Œå°±ç”¨fc_toolçš„æ–¹æ³•
        if self.function_calling_enabled and self.fc_tool:
            result = self.fc_tool.accept_chat(response)

            if self.cut_text_tts and not self.stop_flag:
                self.audio_player.finish_current_text()

            self.ai_is_responding = False

            # AIå›å¤å®Œæˆåï¼Œå¦‚æœæ˜¯éå®æ—¶æ¨¡å¼ï¼Œé‡æ–°å¼€å¯éº¦å…‹é£
            if not self.asr_real_time:
                # ç­‰å¾…æ‰€æœ‰éŸ³é¢‘æ’­æ”¾å®Œæˆ
                self.wait_for_audio_finish()
                self.set_mic_enabled(True)
                print("ğŸ¤ éº¦å…‹é£å·²å¼€å¯ï¼Œå¯ä»¥è¯´è¯äº†")

            print("ğŸ”¥ğŸ”¥ğŸ”¥ AIå›å¤ç»“æŸï¼ğŸ”¥ğŸ”¥ğŸ”¥")
            return result
        else:
            full_assistant = ''
            print('AI:', end='')

            for chunk in response:
                if self.stop_flag:
                    print("ğŸ”¥ æ”¶åˆ°æ‰“æ–­ä¿¡å·ï¼Œåœæ­¢AIå›å¤")
                    break
                if chunk.choices and chunk.choices[0].delta.content is not None:
                    ai_response = chunk.choices[0].delta.content
                    print(ai_response, end='', flush=True)

                    # ğŸ¯ åˆ é™¤æ—§çš„ç«‹å³è§¦å‘ï¼Œç°åœ¨AudioPlayerä¼šè‡ªåŠ¨å¤„ç†æƒ…ç»ªåŒæ­¥
                    # self.emotion_handler.process_text_chunk(ai_response)  # å·²åˆ é™¤

                    # æ ¹æ®configé…ç½®æ–‡ä»¶å¸ƒå°”å€¼åˆ¤æ–­æ˜¯å¦å¼€å¯ttsè¯­éŸ³æ’­æ”¾
                    if self.cut_text_tts:
                        self.audio_player.cut_text(ai_response)

                    full_assistant += ai_response

                    # æµ‹è¯•ç”¨ï¼šç¨å¾®å»¶è¿Ÿä¸€ä¸‹ï¼Œæ–¹ä¾¿æµ‹è¯•æ‰“æ–­
                    time.sleep(0.05)

            # forå¾ªç¯å®Œå…¨ç»“æŸåï¼Œå¹¶ä¸”åªæœ‰åœ¨æ²¡æœ‰è¢«æ‰“æ–­çš„æƒ…å†µä¸‹æ‰å¤„ç†
            if self.cut_text_tts and not self.stop_flag:
                self.audio_player.finish_current_text()

            print()

            # é‡ç½®AIå›å¤çŠ¶æ€
            self.ai_is_responding = False
            self.stop_flag = False

            # AIå›å¤å®Œæˆåï¼Œå¦‚æœæ˜¯éå®æ—¶æ¨¡å¼ï¼Œé‡æ–°å¼€å¯éº¦å…‹é£
            if not self.asr_real_time:
                # ç­‰å¾…æ‰€æœ‰éŸ³é¢‘æ’­æ”¾å®Œæˆ
                self.wait_for_audio_finish()
                self.set_mic_enabled(True)
                print("ğŸ¤ éº¦å…‹é£å·²å¼€å¯ï¼Œå¯ä»¥è¯´è¯äº†")

            print("ğŸ”¥ğŸ”¥ğŸ”¥ AIå›å¤ç»“æŸï¼ğŸ”¥ğŸ”¥ğŸ”¥")

            # å¯¹è¯ç»“æŸåé‡ç½®æƒ…ç»ªå¤„ç†å™¨ç¼“å†²åŒº
            self.emotion_handler.reset_buffer()

            return full_assistant

    def asr_vad_chat(self):
        if self.asr_vad:
            while True:
                print('å¯åŠ¨ASR')
                user = self.vad_input.vad_asr()
                self.stop_flag = False

                # è°ƒç”¨start_chatè€Œä¸æ˜¯ç›´æ¥add_message
                self.start_chat(user)

    def start_chat(self, user):
        self.stop_flag = False
        data = self.bert.vl_bert(user)
        if data == 'æ˜¯':
            image_data = self.get_image_base64()
            self.add_vl_message(user, image_data)
        else:
            self.add_message('user', user)

        response = self.get_requests()
        ai_response = self.accept_chat(response)

        if ai_response:
            self.add_message('assistant', ai_response)

    def main(self):
        sys.exit(start_gui_with_ai(self.start_chat))

    def start_main(self):
        print('å¼€å§‹å¯¹è¯')

        # å¯åŠ¨å¼¹å¹•ç›‘å¬
        self.listener.start_listening()

        while True:
            # è·å–å¼¹å¹•
            chat = self.listener.get_chat()
            if chat:
                user_message = f"å¼¹å¹•æ¶ˆæ¯ï¼š{chat['nickname']}: {chat['text']}"
                nickname = chat['nickname']

                print(f"æ”¶åˆ°å¼¹å¹•: {nickname}: {user_message}")

                # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
                self.add_message('user', user_message)

                # è·å–AIå›å¤
                response = self.get_requests()
                ai_content = self.accept_chat(response)

                # æ·»åŠ AIå›å¤åˆ°å¯¹è¯å†å²
                self.add_message('assistant', ai_content)

            time.sleep(1)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡æ–°å¼¹å¹•

    def auto_chat(self):
        if self.audo_chat:
            while True:
                jiange = self.interval
                time.sleep(jiange)

                user = self.config['api']['auto_content_chat']
                self.add_message('user', user)
                response = self.get_requests()
                ai_response = self.accept_chat(response)

                if ai_response:
                    self.add_message('assistant', ai_response)

    def start_vl_chat(self):
        user = input('ä½ ï¼š')
        image_data = self.get_image_base64()
        self.add_vl_message(user, image_data)
        response = self.get_requests()
        ai_response = self.accept_chat(response)
        if ai_response:
            self.add_message('assistant', ai_response)

    def main_chat(self):
        threading.Thread(target=self.auto_chat, daemon=True).start()
        threading.Thread(target=self.start_main, daemon=True).start()
        threading.Thread(target=self.asr_vad_chat, daemon=True).start()
        # ä¸»çº¿ç¨‹
        if self.config['inputs']['keyboard']['enabled']:
            self.main()
        else:
            while True:
                user = input('ä½ ï¼š')
                self.start_chat(user)


if __name__ == '__main__':
    my_neuro = MyNeuro()
    my_neuro.main_chat()

import json
import re
import random
import threading
import time

# å¯¼å…¥äº‹ä»¶æ€»çº¿
try:
    from UI.simple_event_bus import event_bus, Events
    HAS_EVENT_BUS = True
except ImportError:
    HAS_EVENT_BUS = False

class EmotionHandler:
    """æƒ…ç»ªæ ‡ç­¾å¤„ç†å™¨ - æ”¯æŒæ—¶é—´è½´åŒæ­¥"""

    def __init__(self, config_path="emotion_actions.json", live_model=None):
        self.live_model = live_model
        self.emotion_config = {}
        self.motion_file_to_index = {}
        self.text_buffer = ""
        
        # æ—¶é—´è½´åŒæ­¥ç›¸å…³
        self.emotion_timeline = []
        self.current_text_segment = ""
        self.sync_timer = None
        self.audio_start_time = None

        # åŠ è½½é…ç½®
        self.load_config(config_path)
        # æ„å»ºåŠ¨ä½œæ–‡ä»¶ååˆ°ç´¢å¼•çš„æ˜ å°„
        self.build_motion_mapping()

    def load_config(self, config_path):
        """åŠ è½½æƒ…ç»ªé…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                self.emotion_config = config.get("emotion_actions", {})
        except Exception as e:
            self.emotion_config = {}

    def build_motion_mapping(self):
        """æ„å»ºåŠ¨ä½œæ–‡ä»¶ååˆ°ç´¢å¼•çš„æ˜ å°„"""
        motion_files = [
            "Hiyori_m02.motion3.json",  # index 0
            "Hiyori_m03.motion3.json",  # index 1
            "Hiyori_m04.motion3.json",  # index 2
            "Hiyori_m05.motion3.json",  # index 3
            "Hiyori_m06.motion3.json",  # index 4
            "Hiyori_m07.motion3.json",  # index 5
            "Hiyori_m08.motion3.json",  # index 6
            "Hiyori_m09.motion3.json",  # index 7
            "micoff.motion3.json",      # index 8
            "micon.motion3.json"        # index 9
        ]

        for index, filename in enumerate(motion_files):
            self.motion_file_to_index[filename] = index

    def prepare_text_for_audio(self, text):
        """ä¸ºéŸ³é¢‘æ’­æ”¾å‡†å¤‡æ–‡æœ¬ï¼Œæå–æƒ…ç»ªæ ‡ç­¾å’Œä½ç½®ä¿¡æ¯"""
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ‰€æœ‰æƒ…ç»ªæ ‡ç­¾å’Œä½ç½®
        pattern = r'<([^>]+)>'
        emotion_tags = []
        
        # æ‰¾åˆ°æ‰€æœ‰åŒ¹é…é¡¹
        for match in re.finditer(pattern, text):
            emotion = match.group(1)
            if emotion in self.emotion_config:
                emotion_tags.append({
                    'emotion': emotion,
                    'start_index': match.start(),
                    'end_index': match.end(),
                    'full_tag': match.group(0)
                })
        
        # å¦‚æœæ²¡æœ‰æƒ…ç»ªæ ‡ç­¾ï¼Œç›´æ¥è¿”å›
        if not emotion_tags:
            return {
                'clean_text': text,
                'emotion_markers': []
            }
        
        # åˆ›å»ºçº¯æ–‡æœ¬ç‰ˆæœ¬ï¼ˆç§»é™¤æ‰€æœ‰æƒ…ç»ªæ ‡ç­¾ï¼‰
        clean_text = text
        # ä»åå‘å‰å¤„ç†ï¼Œé¿å…ä½ç½®åç§»é—®é¢˜
        for tag in reversed(emotion_tags):
            clean_text = clean_text[:tag['start_index']] + clean_text[tag['end_index']:]
        
        # åˆ›å»ºæƒ…ç»ªæ ‡è®°æ•°ç»„ï¼Œè½¬æ¢ä¸ºåŸºäºå­—ç¬¦ä½ç½®çš„æ ‡è®°
        emotion_markers = []
        offset = 0
        
        for tag in emotion_tags:
            # è®¡ç®—åœ¨çº¯æ–‡æœ¬ä¸­çš„ä½ç½®ï¼ˆè€ƒè™‘å‰é¢ç§»é™¤çš„æ ‡ç­¾ï¼‰
            adjusted_position = tag['start_index'] - offset
            offset += tag['end_index'] - tag['start_index']
            
            # è·å–å¯¹åº”çš„åŠ¨ä½œ
            motion_files = self.emotion_config[tag['emotion']]
            if motion_files:
                # éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œæ–‡ä»¶
                selected_motion_file = random.choice(motion_files)
                motion_index = self.motion_file_to_index.get(selected_motion_file)
                
                if motion_index is not None:
                    emotion_markers.append({
                        'position': adjusted_position,
                        'emotion': tag['emotion'],
                        'motion_index': motion_index,
                        'motion_file': selected_motion_file
                    })
        
        return {
            'clean_text': clean_text,
            'emotion_markers': emotion_markers
        }

    def start_audio_sync(self, clean_text, emotion_markers, audio_duration):
        """å¼€å§‹éŸ³é¢‘åŒæ­¥ï¼Œæ ¹æ®éŸ³é¢‘æ—¶é•¿å’Œæ–‡æœ¬é•¿åº¦è®¡ç®—è§¦å‘æ—¶æœº"""
        if not emotion_markers:
            return
        
        self.current_text_segment = clean_text
        self.emotion_timeline = []
        self.audio_start_time = time.time()
        
        # è®¡ç®—æ¯ä¸ªå­—ç¬¦å¯¹åº”çš„æ—¶é—´
        text_length = len(clean_text)
        if text_length == 0:
            return
        
        char_duration = audio_duration / text_length
        
        # ä¸ºæ¯ä¸ªæƒ…ç»ªæ ‡è®°è®¡ç®—è§¦å‘æ—¶é—´
        for marker in emotion_markers:
            trigger_time = marker['position'] * char_duration
            
            self.emotion_timeline.append({
                'trigger_time': trigger_time,
                'emotion': marker['emotion'],
                'motion_index': marker['motion_index'],
                'motion_file': marker['motion_file'],
                'triggered': False
            })
        
        # å¯åŠ¨åŒæ­¥è®¡æ—¶å™¨
        self.start_sync_timer()

    def start_sync_timer(self):
        """å¯åŠ¨åŒæ­¥è®¡æ—¶å™¨"""
        if self.sync_timer:
            self.sync_timer.cancel()
        
        def check_triggers():
            if not self.audio_start_time or not self.emotion_timeline:
                return
            
            current_time = time.time()
            elapsed_time = current_time - self.audio_start_time
            
            # æ£€æŸ¥æ˜¯å¦æœ‰éœ€è¦è§¦å‘çš„æƒ…ç»ªåŠ¨ä½œ
            for timeline_item in self.emotion_timeline:
                if (not timeline_item['triggered'] and 
                    elapsed_time >= timeline_item['trigger_time']):
                    
                    # è§¦å‘åŠ¨ä½œ
                    self.trigger_emotion_motion(
                        timeline_item['emotion'],
                        timeline_item['motion_index'],
                        timeline_item['motion_file']
                    )
                    timeline_item['triggered'] = True
            
            # ç»§ç»­æ£€æŸ¥ï¼ˆæ¯50msæ£€æŸ¥ä¸€æ¬¡ï¼‰
            if any(not item['triggered'] for item in self.emotion_timeline):
                self.sync_timer = threading.Timer(0.05, check_triggers)
                self.sync_timer.start()
        
        # å¼€å§‹æ£€æŸ¥
        check_triggers()

    def trigger_emotion_motion(self, emotion, motion_index, motion_file):
        """è§¦å‘æƒ…ç»ªåŠ¨ä½œ"""
        if self.live_model:
            try:
                self.live_model.play_tapbody_motion(motion_index)
            except Exception as e:
                pass
        
        # ğŸ†• å‘å¸ƒæƒ…ç»ªè§¦å‘äº‹ä»¶ï¼Œç”¨äºå¿ƒæƒ…é¢œè‰²å˜åŒ–
        if HAS_EVENT_BUS:
            event_bus.publish("emotion_triggered", {
                "emotion": emotion,
                "motion_index": motion_index,
                "motion_file": motion_file,
                "timestamp": time.time()
            })
            
        print(f"ğŸ­ è§¦å‘æƒ…ç»ª: {emotion}")

    def stop_audio_sync(self):
        """åœæ­¢éŸ³é¢‘åŒæ­¥"""
        if self.sync_timer:
            self.sync_timer.cancel()
            self.sync_timer = None
        
        self.emotion_timeline = []
        self.audio_start_time = None

    def process_text_chunk(self, text_chunk):
        """å¤„ç†æµå¼æ–‡æœ¬å—ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼Œä½†ä¸ä¼šç«‹å³è§¦å‘ï¼‰"""
        if not text_chunk:
            return
        self.text_buffer += text_chunk

    def reset_buffer(self):
        """é‡ç½®æ‰€æœ‰ç¼“å†²åŒºå’ŒçŠ¶æ€"""
        self.text_buffer = ""
        self.stop_audio_sync()

    def get_available_emotions(self):
        """è·å–å¯ç”¨çš„æƒ…ç»ªåˆ—è¡¨"""
        return list(self.emotion_config.keys())
